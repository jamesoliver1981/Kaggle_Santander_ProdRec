colnames(train4)[162]
colnames(train4)[183]
train4$DiffProds_minus12<-rowSums(train4[,183:206])
train5<-select(train4,-c(52:75,95:118,139:162,183:206))
?rename
train5<-rename(train5,ExistingCust_at_pr6=ExistingCust_at_pre6)
train5<-rename(train5,"ExistingCust_at_pr6"="ExistingCust_at_pre6")
colnames(train5$ExistingCust_at_pr6)<-"ExistingCust_at_pre6"
colnames(train5)[76]<-"ExistingCust_at_pre6"
colnames(train5)[119]
train5<-select(train4,-c(52:75,95:118,139:162,183:206))
colnames(train5)
colnames(train5)[52]
colnames(train5)[52]<-"ExistingCust_at_pre6"
colnames(train5)[71]
colnames(train5)[71]<-"ExistingCust_at_pre9"
colnames(train5)[71]
train5<-train5 %>% select(-contains ("pr_6"))
train5<-train5 %>% select(-contains ("pr_9"))
train_label<-train5$Changed
train6<-select(train5,-Changed)
trainmatrix<-data.matrix(train6)
dtrain <- xgb.DMatrix(data = trainmatrix, label= train_label)
saveRDS(dtrain,"train_xgb.rds")
str(train6)
View(str(train6))
which(colnames(train6)=="indfallS")
str(train6,list.len=ncol(train6))
train5[] <- lapply(train5, as.numeric)
str(train5,list.len=ncol(train5))
train_label<-train5$Changed
train6<-select(train5,-Changed)
trainmatrix<-data.matrix(train6)
dtrain <- xgb.DMatrix(data = trainmatrix, label= train_label)
saveRDS(dtrain,"train_xgb.rds")
#reduce data size to allow for computation
#randomly select 100,000 cases, check proportions
#leave proportion in 1:20 ish
set.seed(7463)
random<-sample(1:100,nrow(train3),replace=TRUE)
train3$random<-random
#randomly select approx 100k cases
train4<-train3[train3$random %in% 25:28,]
#randomly select approx 100k cases
train4<-train3[train3$random %in% 25:26,]
#quick double check on proportions: OK
table(train4$Changed)
train4$DiffProds_minus3<-rowSums(train4[,52:75])
train4$DiffProds_minus6<-rowSums(train4[,95:118])
train4$DiffProds_minus9<-rowSums(train4[,139:162])
train4$DiffProds_minus12<-rowSums(train4[,183:206])
train5<-select(train4,-c(52:75,95:118,139:162,183:206))
colnames(train5)[52]<-"ExistingCust_at_pre6"
colnames(train5)[71]<-"ExistingCust_at_pre9"
train5<-train5 %>% select(-contains ("pr_6"))
train5<-train5 %>% select(-contains ("pr_9"))
train5[] <- lapply(train5, as.numeric)
train_label<-train5$Changed
train6<-select(train5,-Changed)
trainmatrix<-data.matrix(train6)
dtrain <- xgb.DMatrix(data = trainmatrix, label= train_label)
model <- xgboost(data = dtrain, # the data
nround = 2, # max number of boosting iterations
objective = "binary:logistic")  # the objective function
model <- xgboost(data = dtrain, # the data
nround = 6, # max number of boosting iterations
objective = "binary:logistic")  # the objective function
model <- xgboost(data = dtrain, # the data
nround = 10, # max number of boosting iterations
objective = "binary:logistic")  # the objective function
model
train3<-cbind(nums,fact2)
rm(trainmatrix)
rm(train6, train5)
rm(dtrain)
rm(train3)
train3<-cbind(nums,fact2)
train3<-train3[train3$Changed %in% c(0,1),]
#Reducing to try to improve performance
train3$DiffProds_minus3<-rowSums(train3[,52:75])
train3$DiffProds_minus6<-rowSums(train3[,95:118])
train3$DiffProds_minus9<-rowSums(train3[,139:162])
train3$DiffProds_minus12<-rowSums(train3[,183:206])
train4<-select(train3,-c(52:75,95:118,139:162,183:206))
colnames(train4)[52]<-"ExistingCust_at_pre6"
colnames(train4)[71]<-"ExistingCust_at_pre9"
train4<-train %>% select(-contains ("pr_6"))
train4<-train4 %>% select(-contains ("pr_9"))
train4[] <- lapply(train4, as.numeric)
train4<-select(train3,-c(52:75,95:118,139:162,183:206))
colnames(train4)[52]<-"ExistingCust_at_pre6"
colnames(train4)[71]<-"ExistingCust_at_pre9"
train4<-train4 %>% select(-contains ("pr_6"))
train4<-train4 %>% select(-contains ("pr_9"))
train4[] <- lapply(train4, as.numeric)
set.seed(7463)
random<-sample(1:100,nrow(train4),replace=TRUE)
train4$random<-random
#randomly select approx 100k cases
train5<-train4[train4$random %in% 25:26,]
#quick double check on proportions: OK
table(train6$Changed)
train_label<-train5$Changed
train6<-select(train5,-c(Changed,random) )
trainmatrix<-data.matrix(train6)
colnames(train6)
dtrain <- xgb.DMatrix(data = trainmatrix, label= train_label)
model <- xgboost(data = dtrain, # the data
nround = 5, # max number of boosting iterations
objective = "binary:logistic")  # the objective function
test<-train4[train4$random %in% 80:100,]
test_label<-test$Changed
test<-select(test,-c(Changed,random) )
testmatrix<-data.matrix(test)
dtest <- xgb.DMatrix(data = test, label= test_label)
head(test_label)
str(test_label)
table(test_label)
str(train_label)
dtest <- xgb.DMatrix(data = testmatrix, label= test_label)
pred <- predict(model, dtest)
table(pred)
dim(pred)
str(pred)
results<-data.frame(TestData = test_label,Prediction=pred)
View(head(pred))
View(head(results))
results$PredClass<-as.numeric(results$Prediction > 0.50)
View(head(results))
library((caret))
library(caret)
confusionMatrix(results$TestData,results$PredClass)
confusionMatrix(as.factor(results$TestData),as.factor(results$PredClass))
confusionMatrix(as.factor(results$PredClass),as.factor(results$TestData))
#rebalancing data
negative_cases <- sum(train_labels == 0)
#rebalancing data
negative_cases <- sum(train_label == 0)
postive_cases <- sum(train_label == 1)
negative_cases/postive_cases
xgb2 <- xgboost(data = dtrain, # the data
nround = 5, # max number of boosting iterations
objective = "binary:logistic",
scale_pos_weight = negative_cases/postive_cases)
xgb2 <- xgboost(data = dtrain, # the data
nround = 10, # max number of boosting iterations
objective = "binary:logistic",
scale_pos_weight = negative_cases/postive_cases)
xgb2 <- xgboost(data = dtrain, # the data
nround = 20, # max number of boosting iterations
objective = "binary:logistic",
scale_pos_weight = negative_cases/postive_cases)
gen_results<-funtion(mod, testset=dtest,cutoff=0.5){
pred <- predict(model, testset)
results<-data.frame(TestData = test_label,Prediction=pred)
results$PredClass<-as.numeric(results$Prediction > cutoff)
confusionMatrix(as.factor(results$PredClass),as.factor(results$TestData))
}
gen_results<-function(mod, testset=dtest,cutoff=0.5){
pred <- predict(model, testset)
results<-data.frame(TestData = test_label,Prediction=pred)
results$PredClass<-as.numeric(results$Prediction > cutoff)
confusionMatrix(as.factor(results$PredClass),as.factor(results$TestData))
}
gen_results(xgb2,dtest,0.5)
xgb2 <- xgboost(data = dtrain, # the data
nround = 50, # max number of boosting iterations
objective = "binary:logistic",
scale_pos_weight = negative_cases/postive_cases)
gen_results(xgb2,dtest,0.5)
xgb2 <- xgboost(data = dtrain, # the data
nround = 50, # max number of boosting iterations
objective = "binary:logistic",
scale_pos_weight = negative_cases*2/postive_cases)
gen_results(xgb2,dtest,0.5)
xgb2 <- xgboost(data = dtrain, # the data
nround = 50, # max number of boosting iterations
objective = "binary:logistic",
scale_pos_weight = negative_cases/postive_cases)
gen_results(xgb2,dtest,0.3)
gen_results(xgb2,dtest,0.2)
xgb2 <- xgboost(data = dtrain, # the data
nround = 100, # max number of boosting iterations
objective = "binary:logistic",
scale_pos_weight = negative_cases/postive_cases)
gen_results(xgb2,dtest,0.5)
gen_results(xgb2,dtest,0.2)
xgb2 <- xgboost(data = dtrain, # the data
max.depth = 10,
nround = 100, # max number of boosting iterations
objective = "binary:logistic",
scale_pos_weight = negative_cases/postive_cases)
gen_results(xgb2,dtest,0.2)
gen_results<-function(mod, testset=dtest,cutoff=0.5){
pred <- predict(mod testset)
results<-data.frame(TestData = test_label,Prediction=pred)
results$PredClass<-as.numeric(results$Prediction > cutoff)
confusionMatrix(as.factor(results$PredClass),as.factor(results$TestData))
}
gen_results<-function(mod, testset=dtest,cutoff=0.5){
pred <- predict(mod testset)
results<-data.frame(TestData = test_label,Prediction=pred)
results$PredClass<-as.numeric(results$Prediction > cutoff)
confusionMatrix(as.factor(results$PredClass),as.factor(results$TestData))
}
gen_results<-function(mod, testset=dtest,cutoff=0.5){
pred <- predict(mod, testset)
results<-data.frame(TestData = test_label,Prediction=pred)
results$PredClass<-as.numeric(results$Prediction > cutoff)
confusionMatrix(as.factor(results$PredClass),as.factor(results$TestData))
}
gen_results(xgb2,dtest,0.2)
gen_results(xgb2,dtest,0.5)
xgb2 <- xgboost(data = dtrain, # the data
max.depth = 6,
nround = 100, # max number of boosting iterations
objective = "binary:logistic",
scale_pos_weight = negative_cases/postive_cases)
gen_results(xgb2,dtest,0.5)
xgb2 <- xgboost(data = dtrain, # the data
max.depth = 10,
nround = 100, # max number of boosting iterations
objective = "binary:logistic",
scale_pos_weight = negative_cases/postive_cases)
gen_results(xgb2,dtest,0.5)
xgb3 <- xgboost(data = dtrain, # the data
max.depth = 10,
nround = 100, # max number of boosting iterations
objective = "binary:logistic",
scale_pos_weight = negative_cases*2/postive_cases)
gen_results(xgb2,dtest,0.5)
gen_results(xgb3,dtest,0.5)
xgb4 <- xgboost(data = dtrain, # the data
max.depth = 10,
nround = 100, # max number of boosting iterations
objective = "binary:logistic",
scale_pos_weight = negative_cases*5/postive_cases)
gen_results(xgb4,dtest,0.5)
gen_results(xgb3,dtest,0.5)
1096/(1096+14458)
1754/(1754+25194)
gen_results(xgb2,dtest,0.5)
704/(704+9124)
xgb5 <- xgboost(data = dtrain, # the data
max.depth = 15,
nround = 150, # max number of boosting iterations
objective = "binary:logistic",
scale_pos_weight = negative_cases*2/postive_cases)
gen_results(xgb5,dtest,0.5)
# colnames(train4)[52]<-"ExistingCust_at_pre6"
# colnames(train4)[71]<-"ExistingCust_at_pre9"
#
# train4<-train4 %>% select(-contains ("pr_6"))
# train4<-train4 %>% select(-contains ("pr_9"))
train4<-train3
train4[] <- lapply(train4, as.numeric)
set.seed(7463)
random<-sample(1:100,nrow(train4),replace=TRUE)
train4$random<-random
#randomly select approx 100k cases
train5<-train4[train4$random %in% 25:26,]
#quick double check on proportions: OK
table(train6$Changed)
train_label<-train5$Changed
train6<-select(train5,-c(Changed,random) )
trainmatrix<-data.matrix(train6)
dtrain <- xgb.DMatrix(data = trainmatrix, label= train_label)
set.seed(7896)
set.seed(7896)
xgb5 <- xgboost(data = dtrain, # the data
max.depth = 10,
nround = 100, # max number of boosting iterations
objective = "binary:logistic",
scale_pos_weight = negative_cases*2/postive_cases)
gen_results(xgb5,dtest,0.5)
test<-train4[train4$random %in% 80:100,]
test_label<-test$Changed
test<-select(test,-c(Changed,random) )
testmatrix<-data.matrix(test)
dtest <- xgb.DMatrix(data = testmatrix, label= test_label)
gen_results<-function(mod, testset=dtest,cutoff=0.5){
pred <- predict(mod, testset)
results<-data.frame(TestData = test_label,Prediction=pred)
results$PredClass<-as.numeric(results$Prediction > cutoff)
table(as.factor(results$PredClass),as.factor(results$TestData))
}
gen_results(xgb5,dtest,0.5)
1167/(1167+15814)
gen_results(xgb5,dtest,0.5)
gen_results<-function(mod, testset=dtest,cutoff=0.5){
pred <- predict(mod, testset)
results<-data.frame(TestData = test_label,Prediction=pred)
results$PredClass<-as.numeric(results$Prediction > cutoff)
confusionMatrix(as.factor(results$PredClass),as.factor(results$TestData))
#table(as.factor(results$PredClass),as.factor(results$TestData))
}
gen_results(xgb5,dtest,0.5)
gen_results(xgb5,dtest,0.2)
gen_results<-function(mod, testset=dtest,cutoff=0.5){
pred <- predict(mod, testset)
results<-data.frame(TestData = test_label,Prediction=pred)
results$PredClass<-as.numeric(results$Prediction > cutoff)
confusionMatrix(as.factor(results$PredClass),as.factor(results$TestData),positive = 1)
#table(as.factor(results$PredClass),as.factor(results$TestData))
}
gen_results(xgb5,dtest,0.2)
gen_results<-function(mod, testset=dtest,cutoff=0.5){
pred <- predict(mod, testset)
results<-data.frame(TestData = test_label,Prediction=pred)
results$PredClass<-as.numeric(results$Prediction > cutoff)
confusionMatrix(as.factor(results$PredClass),as.factor(results$TestData),positive = "1")
#table(as.factor(results$PredClass),as.factor(results$TestData))
}
gen_results(xgb5,dtest,0.2)
53537/(53537+130513)
gen_results(xgb5,dtest,0.5)
set.seed(7896)
xgb6 <- xgboost(data = dtrain, # the data
max.depth = 10,
nround = 100, # max number of boosting iterations
objective = "binary:logistic",
scale_pos_weight = negative_cases*10/postive_cases)
gen_results(xgb5,dtest,0.5)
gen_results(xgb6,dtest,0.5)
set.seed(7896)
xgb7 <- xgboost(data = dtrain, # the data
max.depth = 10,
nround = 100, # max number of boosting iterations
objective = "binary:logistic",
scale_pos_weight = negative_cases*10/postive_cases,
gamma = 1) # add a regularization term
gen_results(xgb7,dtest,0.5)
28352+1878/(28352+1878+155698+8191)
(28352+1878)/(28352+1878+155698+8191)
gen_results(xgb2,dtest,0.5)
?assign
gen_results<-function(mod, testset=dtest,cutoff=0.5){
pred <- predict(mod, testset)
results<-data.frame(TestData = test_label,Prediction=pred)
results$PredClass<-as.numeric(results$Prediction > cutoff)
confusionMatrix(as.factor(results$PredClass),as.factor(results$TestData),positive = "1")
assign(paste("out_",mod))<-confusionMatrix(as.factor(results$PredClass),as.factor(results$TestData),positive = "1")
#table(as.factor(results$PredClass),as.factor(results$TestData))
}
gen_results(xgb2,dtest,0.5)
gen_results(xgb7,dtest,0.5)
names(xgb7)
?deparse
gen_results<-function(mod, testset=dtest,cutoff=0.5){
pred <- predict(mod, testset)
results<-data.frame(TestData = test_label,Prediction=pred)
results$PredClass<-as.numeric(results$Prediction > cutoff)
confusionMatrix(as.factor(results$PredClass),as.factor(results$TestData),positive = "1")
assign(paste("out",deparse(substitute(mod))),
confusionMatrix(as.factor(results$PredClass),as.factor(results$TestData),positive = "1"),
envir=.GlobalEnv)
#table(as.factor(results$PredClass),as.factor(results$TestData))
}
gen_results(xgb7,dtest,0.5)
`out xgb7`
gen_results<-function(mod, testset=dtest,cutoff=0.5){
pred <- predict(mod, testset)
results<-data.frame(TestData = test_label,Prediction=pred)
results$PredClass<-as.numeric(results$Prediction > cutoff)
confusionMatrix(as.factor(results$PredClass),as.factor(results$TestData),positive = "1")
assign(paste("out_",deparse(substitute(mod)),sep=""),
confusionMatrix(as.factor(results$PredClass),as.factor(results$TestData),positive = "1"),
envir=.GlobalEnv)
#table(as.factor(results$PredClass),as.factor(results$TestData))
}
gen_results(xgb7,dtest,0.5)
gen_results(xgb5,dtest,0.5)
gen_results(xgb6,dtest,0.5)
gen_results<-function(mod, testset=dtest,cutoff=0.5){
pred <- predict(mod, testset)
results<-data.frame(TestData = test_label,Prediction=pred)
results$PredClass<-as.numeric(results$Prediction > cutoff)
assign(paste("out_",deparse(substitute(mod)),sep=""),
confusionMatrix(as.factor(results$PredClass),as.factor(results$TestData),positive = "1"),
envir=.GlobalEnv)
confusionMatrix(as.factor(results$PredClass),as.factor(results$TestData),positive = "1")
}
gen_results(xgb5,dtest,0.5)
gen_results(xgb6,dtest,0.5)
(2363+36148)/(2363+36148+147902+7706)
(11731+215566)/(11731+215566+660398+37382)
(11731)/(11731+215566)
(11731+37382)/(11731+215566+660398+37382)
11731/(11731+37382)
set.seed(7896)
xgb8 <- xgboost(data = dtrain, # the data
max.depth = 10,
nround = 100, # max number of boosting iterations
objective = "binary:logistic",
scale_pos_weight = negative_cases/postive_cases)
gen_results(xgb8,dtest,0.5)
gen_results(xgb8,dtest,0.4)
set.seed(7896)
xgb9 <- xgboost(data = dtrain, # the data
max.depth = 10,
nround = 100, # max number of boosting iterations
objective = "binary:logistic",
scale_pos_weight = (negative_cases*0.75)/postive_cases)
gen_results(xgb9,dtest,0.4)
set.seed(7896)
xgb10 <- xgboost(data = dtrain, # the data
max.depth = 10,
nround = 100, # max number of boosting iterations
objective = "binary:logistic",
scale_pos_weight = (negative_cases*0.5)/postive_cases)
gen_results(xgb10,dtest,0.4)
gen_results(xgb10,dtest,0.3)
#does the model improve when we add more data?
train5<-train4[train4$random %in% 25:34,]
train5_2<-train4[train4$random %in% 25:34,]
train_label_2<-train5_2$Changed
train6_2<-select(train5_2,-c(Changed,random) )
trainmatrix_2<-data.matrix(train6_2)
dtrain <- xgb.DMatrix(data = trainmatrix_2, label= train_label_2)
set.seed(7896)
xgb10_2 <- xgboost(data = dtrain_2, # the data
max.depth = 10,
nround = 100, # max number of boosting iterations
objective = "binary:logistic",
scale_pos_weight = (negative_cases*0.5)/postive_cases)
gen_results(xgb10_2,dtest,0.4)
dtrain_2 <- xgb.DMatrix(data = trainmatrix_2, label= train_label_2)
set.seed(7896)
xgb10_2 <- xgboost(data = dtrain_2, # the data
max.depth = 10,
nround = 100, # max number of boosting iterations
objective = "binary:logistic",
scale_pos_weight = (negative_cases*0.5)/postive_cases)
gen_results(xgb10_2,dtest,0.4)
#adding even more data
train5_3<-train4[train4$random %in% 25:45,]
train_label_3<-train5_3$Changed
train6_3<-select(train5_3,-c(Changed,random) )
trainmatrix_3<-data.matrix(train6_3)
dtrain_3 <- xgb.DMatrix(data = trainmatrix_3, label= train_label_3)
set.seed(7896)
xgb10_3 <- xgboost(data = dtrain_3, # the data
max.depth = 10,
nround = 100, # max number of boosting iterations
objective = "binary:logistic",
scale_pos_weight = (negative_cases*0.5)/postive_cases)
gen_results(xgb10_3,dtest,0.4)
gen_results<-function(mod, testset=dtest,cutoff=0.5){
pred <- predict(mod, testset)
#Overall error rate
err <- mean(as.numeric(pred > 0.5) != test_label)
print(paste("test-error=", err))
results<-data.frame(TestData = test_label,Prediction=pred)
results$PredClass<-as.numeric(results$Prediction > cutoff)
assign(paste("out_",deparse(substitute(mod)),sep=""),
confusionMatrix(as.factor(results$PredClass),as.factor(results$TestData),positive = "1"),
envir=.GlobalEnv)
confusionMatrix(as.factor(results$PredClass),as.factor(results$TestData),positive = "1")
}
gen_results(xgb10_3,dtest,0.4)
set.seed(7896)
xgb11_3 <- xgboost(data = dtrain_3, # the data
max.depth = 10,
nround = 150, # max number of boosting iterations
objective = "binary:logistic",
scale_pos_weight = (negative_cases*0.5)/postive_cases)
gen_results(xgb11_3,dtest,0.4)
# get information on how important each feature is
importance_matrix <- xgb.importance(names(trainmatrix_3), model = xgb10_3)
# and plot it!
xgb.plot.importance(importance_matrix)
head(importance_matrix)
saveRDS(xgb10_3,"xgb_10d_100n_halfweight_194k.rds")
# and plot it!
xgb.plot.importance(head(importance_matrix),20)
# and plot it!
xgb.plot.importance(head(importance_matrix,20))
?xgb.plot.importance
# and plot it!
xgb.plot.importance(head(importance_matrix,20))+ggtitle("Importance Plot of XGBoost")
# and plot it!
pl<-xgb.plot.importance(head(importance_matrix,20))
pl+ggtitle("Importance Plot of XGBoost")
pl
# and plot it!
xgb.plot.importance(head(importance_matrix,15))
pl<-xgb.plot.importance(head(importance_matrix,17))
ggtitle("Importance Plot of XGBoost")
+ggtitle("Importance Plot of XGBoost")
pl+ggtitle("Importance Plot of XGBoost")
pl<-xgb.ggplot.importance(head(importance_matrix,17))
pl+ggtitle("Importance Plot of XGBoost")
pl<-xgb.ggplot.importance(head(importance_matrix,20))
pl+ggtitle("Importance Plot of XGBoost")
pl<-xgb.ggplot.importance(head(importance_matrix,15))
pl+ggtitle("Importance Plot of XGBoost")
set.seed(7896)
xgb1 <- xgboost(data = dtrain, # the data
max.depth = 10,
nround = 100, # max number of boosting iterations
objective = "binary:logistic")
gen_results(xgb1,dtest,0.5)
set.seed(7896)
xgb2 <- xgboost(data = dtrain, # the data
max.depth = 10,
nround = 100, # max number of boosting iterations
objective = "binary:logistic",
scale_pos_weight = negative_cases/postive_cases)
gen_results(xgb2,dtest,0.5)
#sens 0,2%
#Prev Detection 5,2%
#Pos Pred Value 24,6%
